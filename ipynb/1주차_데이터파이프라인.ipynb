{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe4929a-7a32-4bfe-9c0e-b130306850e7",
   "metadata": {},
   "source": [
    "### 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfb7997e-0687-45a7-83d2-7ea55ab252d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/kihyunwon/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/kihyunwon/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/kihyunwon/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kihyunwon/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/kihyunwon/Library/Python/3.9/lib/python/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a967ba2c-e134-46e2-8127-86ac973c3fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kihyunwon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/kihyunwon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kihyunwon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b3c2d5-cdad-46b1-8220-e9686dced603",
   "metadata": {},
   "source": [
    "### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1003cb7f-c4ff-4de2-8fea-7e3bbe12fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Nvidia's Huang says nuclear power an option to feed data warehouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "425f2ebc-6382-4ea9-9685-061d2824c6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Nvidia's\",\n",
       " 'Huang',\n",
       " 'says',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'an',\n",
       " 'option',\n",
       " 'to',\n",
       " 'feed',\n",
       " 'data',\n",
       " 'warehouse']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = text.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "380f8037-69c6-414b-95f5-3daee99efa7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nvidia',\n",
       " \"'s\",\n",
       " 'Huang',\n",
       " 'says',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'an',\n",
       " 'option',\n",
       " 'to',\n",
       " 'feed',\n",
       " 'data',\n",
       " 'warehouse']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 이용 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = word_tokenize(text)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36624b6f-d059-4ad6-a6a8-8d4b03c913e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nvidia',\n",
       " \"'s\",\n",
       " 'Huang',\n",
       " 'says',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'option',\n",
       " 'feed',\n",
       " 'data',\n",
       " 'warehouse']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 필터링\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecd51e54-f456-4b57-b8a0-983b1b1603aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nvidia',\n",
       " \"'s\",\n",
       " 'huang',\n",
       " 'say',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'option',\n",
       " 'feed',\n",
       " 'data',\n",
       " 'warehous']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어간 추출\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e4f13b-dad5-4952-8cc2-3ebef8cba56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nvidia',\n",
       " \"'s\",\n",
       " 'Huang',\n",
       " 'say',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'option',\n",
       " 'feed',\n",
       " 'data',\n",
       " 'warehouse']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기본형 추출\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519f020-d968-40ba-a6ec-4f5a98b56f95",
   "metadata": {},
   "source": [
    "### Batch Index Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b45aa-818b-44d3-beba-7f7e94592654",
   "metadata": {},
   "source": [
    "### 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7eebb6-b7bb-4843-8041-9cd0b1d9a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840651 sha256=63b8c5cdebc9ab3f6dac5a31e12873f07e0a10b12f5b0c722343c4883b703afa\n",
      "  Stored in directory: /Users/kihyunwon/Library/Caches/pip/wheels/2e/d2/18/6f4f20e8332359f7fffceb6828edcc80ef96f86744192a7bb9\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "# Skip if you already have Java\n",
    "\n",
    "# # Install Homebrew\n",
    "# /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "\n",
    "# # Install OpenJDK\n",
    "# brew install openjdk@17\n",
    "\n",
    "# # set JAVA_HOME in zshrc\n",
    "# export JAVA_HOME=$(/usr/libexec/java_home -v17)\n",
    "# export PATH=$JAVA_HOME/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d21694-2332-4b7c-891d-05d97b4bf88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if you already have Spark\n",
    "\n",
    "# # Download and install Spark\n",
    "# ! curl -O https://dlcdn.apache.org/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\n",
    "# ! tar -xzf spark-3.4.3-bin-hadoop3.tgz\n",
    "# # Move to /opt folder\n",
    "# ! sudo mv spark-3.4.3-bin-hadoop3 /opt/spark-3.4.3\n",
    "# cp /opt/spark-3.4.3/conf/spark-defaults.conf.template /opt/spark-3.4.3/conf/spark-defaults.conf\n",
    "# echo 'spark.driver.extraJavaOptions   -Djava.security.manager=allow' >> /opt/spark-3.4.3/conf/spark-defaults.conf\n",
    "# echo 'spark.executor.extraJavaOptions   -Djava.security.manager=allow' >> /opt/spark-3.4.3/conf/spark-defaults.conf\n",
    "# # Create symlink\n",
    "# ! sudo ln -s /opt/spark-3.4.3 /opt/spark\n",
    "\n",
    "# # set SPARK_HOME in .zshrc\n",
    "# export SPARK_HOME=/opt/spark\n",
    "# export PATH=$SPARK_HOME/bin:$PATH\n",
    "\n",
    "# # Jupyter Notebook Spark integration\n",
    "# export PYSPARK_PYTHON=$(brew --prefix python)/libexec/bin/python\n",
    "# export PYSPARK_DRIVER_PYTHON='jupyter'\n",
    "# export PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port=8889'\n",
    "# # Download elastic-spark jar from https://central.sonatype.com/artifact/org.elasticsearch/elasticsearch-spark-30_2.12/versions\n",
    "# export PYSPARK_SUBMIT_ARGS='--jars /opt/spark-3.4.3/jars/elasticsearch-spark-30_2.12-8.15.0.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47dde69-64c9-4566-8fa1-e4f9d22af3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if you already have Elasticsearch\n",
    "\n",
    "# Download Docker from https://docs.docker.com/engine/install/\n",
    "\n",
    "# # pull docker images\n",
    "# docker pull docker.elastic.co/elasticsearch/elasticsearch:8.15.0\n",
    "# docker pull docker.elastic.co/kibana/kibana:8.15.0\n",
    "\n",
    "# # start Elasticsearch and Kibana containers\n",
    "# docker network create elastic\n",
    "# docker run -d --name elasticsearch --net elastic -p 9200:9200 -p 9300:9300 -m 1GB -e \"discovery.type=single-node\" -e \"ELASTIC_PASSWORD=password\" -e \"xpack.security.enabled=false\" -e \"xpack.security.enrollment.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.15.0\n",
    "# docker run -d --name kibana --net elastic -p 5601:5601 docker.elastic.co/kibana/kibana:8.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d319687-24c5-4852-ba92-512ade3eb93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+--------------------+-----+\n",
      "|                 _c0|                 _c1|                _c2|                 _c3|                 _c4|  _c5|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+-----+\n",
      "|          COMMENT_ID|              AUTHOR|               DATE|             CONTENT|          VIDEO_NAME|CLASS|\n",
      "|LZQPQhLyRh80UYxNu...|           Julius NM|2013-11-07T06:20:48|Huh, anyway check...|PSY - GANGNAM STY...|    1|\n",
      "|LZQPQhLyRh_C2cTtd...|         adam riyati|2013-11-07T12:37:15|Hey guys check ou...|PSY - GANGNAM STY...|    1|\n",
      "|LZQPQhLyRh9MSZYnf...|    Evgeny Murashkin|2013-11-08T17:34:21|just for test I h...|PSY - GANGNAM STY...|    1|\n",
      "|z13jhp0bxqncu512g...|     ElNino Melendez|2013-11-09T08:28:43|me shaking my sex...|PSY - GANGNAM STY...|    1|\n",
      "|z13fwbwp1oujthgqj...|              GsMega|2013-11-10T16:05:38|watch?v=vtaRGgvGt...|PSY - GANGNAM STY...|    1|\n",
      "|LZQPQhLyRh9-wNRtl...|        Jason Haddad|2013-11-26T02:55:11|Hey, check out my...|PSY - GANGNAM STY...|    1|\n",
      "|z13lfzdo5vmdi1cm1...|      ferleck ferles|2013-11-27T21:39:24|Subscribe to my c...|PSY - GANGNAM STY...|    1|\n",
      "|z122wfnzgt30fhubn...|        Bob Kanowski|2013-11-28T12:33:27|i turned it on mu...|PSY - GANGNAM STY...|    0|\n",
      "|z13ttt1jcraqexk2o...|                Cony|2013-11-28T16:01:47|You should check ...|PSY - GANGNAM STY...|    1|\n",
      "|z12avveb4xqiirsix...|         BeBe Burkey|2013-11-28T16:30:13|and u should.d ch...|PSY - GANGNAM STY...|    1|\n",
      "|z13auhww3oufjn1qo...|           Huckyduck|2013-11-28T17:06:17|Hey subscribe to me﻿|PSY - GANGNAM STY...|    1|\n",
      "|z13xit5agm2zyh4f5...|         Lone Twistt|2013-11-28T17:34:55| Once you have st...|PSY - GANGNAM STY...|    1|\n",
      "|z13pejoiuozwxtdu3...|        Archie Lewis|2013-11-28T17:54:39|https://twitter.c...|PSY - GANGNAM STY...|    1|\n",
      "|z121zxaxsq25z5k5o...|     TheUploadaddict|2013-11-28T18:12:12|subscribe like co...|PSY - GANGNAM STY...|    1|\n",
      "|z12oglnpoq3gjh4om...|      Francisco Nora|2013-11-28T19:52:35|please like :D ht...|PSY - GANGNAM STY...|    1|\n",
      "|z13phrmwrkfisn5er...|Gaming and Stuff PRO|2013-11-28T21:14:13|Hello! Do you lik...|PSY - GANGNAM STY...|    1|\n",
      "|z13bgdvyluihfv11i...|         Zielimeek21|2013-11-28T21:49:00|I'm only checking...|PSY - GANGNAM STY...|    0|\n",
      "|z13vxpnoxsyeuv2jr...|      OutrightIgnite|2013-11-28T21:55:02|http://www.ebay.c...|PSY - GANGNAM STY...|    1|\n",
      "|z12qth5j0ob1fx3q4...|      Tony K Frazier|2013-11-28T23:57:13|http://ubuntuone....|PSY - GANGNAM STY...|    1|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "spark = SparkSession.builder.appName(\"indexer\").config(conf=conf).getOrCreate()\n",
    "\n",
    "# Dataset: https://www.kaggle.com/datasets/ahsenwaheed/youtube-comments-spam-dataset?resource=download\n",
    "path = \"Youtube-Spam-Dataset.csv\"\n",
    "\n",
    "df = spark.read.csv(path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "428d09ef-c90f-4016-ad06-4b1d2744eb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+--------------------+---+\n",
      "|                 _c0|                 _c1|                _c2|                 _c3|                 _c4|_c5|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+---+\n",
      "|LZQPQhLyRh80UYxNu...|           Julius NM|2013-11-07T06:20:48|Huh, anyway check...|PSY - GANGNAM STY...|  1|\n",
      "|LZQPQhLyRh_C2cTtd...|         adam riyati|2013-11-07T12:37:15|Hey guys check ou...|PSY - GANGNAM STY...|  1|\n",
      "|LZQPQhLyRh9MSZYnf...|    Evgeny Murashkin|2013-11-08T17:34:21|just for test I h...|PSY - GANGNAM STY...|  1|\n",
      "|z13jhp0bxqncu512g...|     ElNino Melendez|2013-11-09T08:28:43|me shaking my sex...|PSY - GANGNAM STY...|  1|\n",
      "|z13fwbwp1oujthgqj...|              GsMega|2013-11-10T16:05:38|watch?v=vtaRGgvGt...|PSY - GANGNAM STY...|  1|\n",
      "|LZQPQhLyRh9-wNRtl...|        Jason Haddad|2013-11-26T02:55:11|Hey, check out my...|PSY - GANGNAM STY...|  1|\n",
      "|z13lfzdo5vmdi1cm1...|      ferleck ferles|2013-11-27T21:39:24|Subscribe to my c...|PSY - GANGNAM STY...|  1|\n",
      "|z122wfnzgt30fhubn...|        Bob Kanowski|2013-11-28T12:33:27|i turned it on mu...|PSY - GANGNAM STY...|  0|\n",
      "|z13ttt1jcraqexk2o...|                Cony|2013-11-28T16:01:47|You should check ...|PSY - GANGNAM STY...|  1|\n",
      "|z12avveb4xqiirsix...|         BeBe Burkey|2013-11-28T16:30:13|and u should.d ch...|PSY - GANGNAM STY...|  1|\n",
      "|z13auhww3oufjn1qo...|           Huckyduck|2013-11-28T17:06:17|Hey subscribe to me﻿|PSY - GANGNAM STY...|  1|\n",
      "|z13xit5agm2zyh4f5...|         Lone Twistt|2013-11-28T17:34:55| Once you have st...|PSY - GANGNAM STY...|  1|\n",
      "|z13pejoiuozwxtdu3...|        Archie Lewis|2013-11-28T17:54:39|https://twitter.c...|PSY - GANGNAM STY...|  1|\n",
      "|z121zxaxsq25z5k5o...|     TheUploadaddict|2013-11-28T18:12:12|subscribe like co...|PSY - GANGNAM STY...|  1|\n",
      "|z12oglnpoq3gjh4om...|      Francisco Nora|2013-11-28T19:52:35|please like :D ht...|PSY - GANGNAM STY...|  1|\n",
      "|z13phrmwrkfisn5er...|Gaming and Stuff PRO|2013-11-28T21:14:13|Hello! Do you lik...|PSY - GANGNAM STY...|  1|\n",
      "|z13bgdvyluihfv11i...|         Zielimeek21|2013-11-28T21:49:00|I'm only checking...|PSY - GANGNAM STY...|  0|\n",
      "|z13vxpnoxsyeuv2jr...|      OutrightIgnite|2013-11-28T21:55:02|http://www.ebay.c...|PSY - GANGNAM STY...|  1|\n",
      "|z12qth5j0ob1fx3q4...|      Tony K Frazier|2013-11-28T23:57:13|http://ubuntuone....|PSY - GANGNAM STY...|  1|\n",
      "|z13etj0bclzfztuwc...|       Jose Renteria|2013-11-29T00:22:01|We are an EDM app...|PSY - GANGNAM STY...|  1|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = df.columns\n",
    "new_df = df.filter(df._c0 != 'COMMENT_ID')\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07518291-b6a4-40a2-a3cd-efc61af5d022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+--------------------+-----+\n",
      "|          COMMENT_ID|              AUTHOR|               DATE|             CONTENT|          VIDEO_NAME|CLASS|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+-----+\n",
      "|LZQPQhLyRh80UYxNu...|           Julius NM|2013-11-07T06:20:48|Huh, anyway check...|PSY - GANGNAM STY...|    1|\n",
      "|LZQPQhLyRh_C2cTtd...|         adam riyati|2013-11-07T12:37:15|Hey guys check ou...|PSY - GANGNAM STY...|    1|\n",
      "|LZQPQhLyRh9MSZYnf...|    Evgeny Murashkin|2013-11-08T17:34:21|just for test I h...|PSY - GANGNAM STY...|    1|\n",
      "|z13jhp0bxqncu512g...|     ElNino Melendez|2013-11-09T08:28:43|me shaking my sex...|PSY - GANGNAM STY...|    1|\n",
      "|z13fwbwp1oujthgqj...|              GsMega|2013-11-10T16:05:38|watch?v=vtaRGgvGt...|PSY - GANGNAM STY...|    1|\n",
      "|LZQPQhLyRh9-wNRtl...|        Jason Haddad|2013-11-26T02:55:11|Hey, check out my...|PSY - GANGNAM STY...|    1|\n",
      "|z13lfzdo5vmdi1cm1...|      ferleck ferles|2013-11-27T21:39:24|Subscribe to my c...|PSY - GANGNAM STY...|    1|\n",
      "|z122wfnzgt30fhubn...|        Bob Kanowski|2013-11-28T12:33:27|i turned it on mu...|PSY - GANGNAM STY...|    0|\n",
      "|z13ttt1jcraqexk2o...|                Cony|2013-11-28T16:01:47|You should check ...|PSY - GANGNAM STY...|    1|\n",
      "|z12avveb4xqiirsix...|         BeBe Burkey|2013-11-28T16:30:13|and u should.d ch...|PSY - GANGNAM STY...|    1|\n",
      "|z13auhww3oufjn1qo...|           Huckyduck|2013-11-28T17:06:17|Hey subscribe to me﻿|PSY - GANGNAM STY...|    1|\n",
      "|z13xit5agm2zyh4f5...|         Lone Twistt|2013-11-28T17:34:55| Once you have st...|PSY - GANGNAM STY...|    1|\n",
      "|z13pejoiuozwxtdu3...|        Archie Lewis|2013-11-28T17:54:39|https://twitter.c...|PSY - GANGNAM STY...|    1|\n",
      "|z121zxaxsq25z5k5o...|     TheUploadaddict|2013-11-28T18:12:12|subscribe like co...|PSY - GANGNAM STY...|    1|\n",
      "|z12oglnpoq3gjh4om...|      Francisco Nora|2013-11-28T19:52:35|please like :D ht...|PSY - GANGNAM STY...|    1|\n",
      "|z13phrmwrkfisn5er...|Gaming and Stuff PRO|2013-11-28T21:14:13|Hello! Do you lik...|PSY - GANGNAM STY...|    1|\n",
      "|z13bgdvyluihfv11i...|         Zielimeek21|2013-11-28T21:49:00|I'm only checking...|PSY - GANGNAM STY...|    0|\n",
      "|z13vxpnoxsyeuv2jr...|      OutrightIgnite|2013-11-28T21:55:02|http://www.ebay.c...|PSY - GANGNAM STY...|    1|\n",
      "|z12qth5j0ob1fx3q4...|      Tony K Frazier|2013-11-28T23:57:13|http://ubuntuone....|PSY - GANGNAM STY...|    1|\n",
      "|z13etj0bclzfztuwc...|       Jose Renteria|2013-11-29T00:22:01|We are an EDM app...|PSY - GANGNAM STY...|    1|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_schema = df.schema\n",
    "\n",
    "new_columns = []\n",
    "for i in df.limit(1).collect()[0]:\n",
    "  new_columns.append(i)\n",
    "\n",
    "for i,k in enumerate(df_schema.fields):\n",
    "  k.name = new_columns[i]\n",
    "\n",
    "# Apply the schema to the original dataframe\n",
    "new_df = spark.createDataFrame(new_df.rdd, df_schema)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc9c97e0-5302-492f-a40b-51bafbc51bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/04 22:26:23 ERROR NetworkClient: Node [172.18.0.2:9200] failed (org.elasticsearch.hadoop.thirdparty.apache.commons.httpclient.ConnectTimeoutException: The host did not accept the connection within timeout of 60000 ms); selected next node [127.0.0.1:9200]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write the DataFrame to an Elasticsearch index\n",
    "es_conf = {\n",
    "    \"es.nodes.discovery\": \"false\",\n",
    "    \"es.nodes.data.only\": \"false\",\n",
    "    \"es.net.http.auth.user\": \"elastic\",\n",
    "    \"es.net.http.auth.pass\": \"password\",\n",
    "    \"es.index.auto.create\": \"true\",\n",
    "    \"es.nodes\": \"http://127.0.0.1\",\n",
    "    \"es.port\": \"9200\",\n",
    "    \"es.mapping.id\": \"COMMENT_ID\",\n",
    "}\n",
    "\n",
    "new_df.write.mode(\"append\") \\\n",
    "        .format('org.elasticsearch.spark.sql') \\\n",
    "        .options(**es_conf) \\\n",
    "        .save(\"youtube\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1691cf74-9ae5-4b24-8887-bb63470754fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"took\":7,\"timed_out\":false,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":579,\"relation\":\"eq\"},\"max_score\":12.020087,\"hits\":[{\"_index\":\"youtube\",\"_id\":\"z123std54m2ozht10232efr5svb4vh0au04\",\"_score\":12.020087,\"_source\":{\"COMMENT_ID\":\"z123std54m2ozht10232efr5svb4vh0au04\",\"AUTHOR\":\"MB LOVE\",\"DATE\":\"2014-08-13T03:04:34\",\"CONTENT\":\"damn, nvm what I said\",\"VIDEO_NAME\":\"Katy Perry - Roar\",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"z13rzhjaxxr3z1t4504ccvlp1uqnddvqoyg0k\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"z13rzhjaxxr3z1t4504ccvlp1uqnddvqoyg0k\",\"AUTHOR\":\"Kate Love\",\"DATE\":\"2015-05-23T01:14:31.812000\",\"CONTENT\":\"Best song﻿\",\"VIDEO_NAME\":\"Eminem - Love The Way You Lie ft. Rihanna\",\"CLASS\":\"0\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc6_onwOgxju-DV6WkqHZEOztCXD04EgEFBU\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc6_onwOgxju-DV6WkqHZEOztCXD04EgEFBU\",\"AUTHOR\":\"Hidden Love\",\"DATE\":\"2013-09-24T14:29:54.072000\",\"CONTENT\":\"Hi. Check out and share our songs.\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc69r15LuL8TDbisnTJ_hf5RfcyJAyoMC5eo\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc69r15LuL8TDbisnTJ_hf5RfcyJAyoMC5eo\",\"AUTHOR\":\"Hidden Love\",\"DATE\":\"2013-08-06T11:40:05.581000\",\"CONTENT\":\"Hi. Check out and share our songs.\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc68Qq98m0mmx4rlprYiD6aYgMb2x3bdupEM\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc68Qq98m0mmx4rlprYiD6aYgMb2x3bdupEM\",\"AUTHOR\":\"Hidden Love\",\"DATE\":\"2013-08-01T09:19:56.654000\",\"CONTENT\":\"Hi. Check out and share our songs.\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc698fLCQzZmRt1CqPhf0L6mchwWG5gsUlzk\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc698fLCQzZmRt1CqPhf0L6mchwWG5gsUlzk\",\"AUTHOR\":\"Hidden Love\",\"DATE\":\"2013-07-31T10:22:02.628000\",\"CONTENT\":\"Hi. Check out and share our songs.\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc68sxLpsBQ1cPk_LLH91SMsRav51KmhIoQw\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc68sxLpsBQ1cPk_LLH91SMsRav51KmhIoQw\",\"AUTHOR\":\"Hidden Love\",\"DATE\":\"2013-07-30T11:22:57.091000\",\"CONTENT\":\"Hi.Check out and share our songs.\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"z13bvpnptyiuzjlr004cijkhguzvjlsjrpw0k\",\"_score\":3.9471169,\"_source\":{\"COMMENT_ID\":\"z13bvpnptyiuzjlr004cijkhguzvjlsjrpw0k\",\"AUTHOR\":\"jen ven\",\"DATE\":\"2015-05-18T18:14:56.234000\",\"CONTENT\":\"Love these guys, love the song!﻿\",\"VIDEO_NAME\":\"LMFAO - Party Rock Anthem ft. Lauren Bennett, GoonRock\",\"CLASS\":\"0\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc69JZ1f3TJQWKMLhmdP0inDDPwOvzMQiFjE\",\"_score\":3.8681548,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc69JZ1f3TJQWKMLhmdP0inDDPwOvzMQiFjE\",\"AUTHOR\":\"Su Xianfeng\",\"DATE\":\"2013-10-05T11:54:10.192000\",\"CONTENT\":\"subscribe now!!!!!! love the song!!!!!! love football!!!\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"z123eldrazr3wlyd222zitmg1y2jin1rf04\",\"_score\":3.821743,\"_source\":{\"COMMENT_ID\":\"z123eldrazr3wlyd222zitmg1y2jin1rf04\",\"AUTHOR\":\"Mahya Roohi\",\"DATE\":\"2015-05-21T08:19:31.099000\",\"CONTENT\":\"love!!!!﻿\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"0\"}}]}}"
     ]
    }
   ],
   "source": [
    "# Verify the index in Kibana http://localhost:5601/app/dev_tools#/console\n",
    "! curl -X GET http://localhost:9200/youtube/_search\\?q\\=MB%20LOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62603e9a-836a-41c5-85e1-03ab07aee4b5",
   "metadata": {},
   "source": [
    "### Streaming Index Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3190f5e-2eb9-4e35-8c4c-98d15f729fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: kafka-python in /opt/homebrew/lib/python3.9/site-packages (2.0.2)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Skip if you already have Kafka\n",
    "\n",
    "# # pull docker images\n",
    "# ! docker pull apache/kafka:3.8.0\n",
    "\n",
    "# Start Kafka and Flink containers\n",
    "# ! docker run -d --name kafka -p 9092:9092 apache/kafka:3.8.0\n",
    "\n",
    "# # Install kafka-python client\n",
    "# ! pip3 install kafka-python\n",
    "\n",
    "# # Download spark-sql-kafka jar from https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10_2.12/3.4.3\n",
    "# # Download kafka-client jar from https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients/3.3.2\n",
    "# export PYSPARK_SUBMIT_ARGS='--jars /opt/spark-3.4.3/jars/spark-sql-kafka-0-10_2.12-3.4.3.jar,/opt/spark-3.4.3/jars/kafka-clients-3.3.2.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9231a6fc-de5a-49f9-8067-a5b98128abd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message produced with the offset: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def on_success(metadata):\n",
    "    print(f\"Message produced with the offset: {metadata.offset}\")\n",
    "\n",
    "def on_error(error):\n",
    "    print(f\"An error occurred while publishing the message. {error}\")\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "message = {\n",
    "    \"COMMENT_ID\" : \"z123std54m2ozht10232efr5svb4vh0au04\",\n",
    "    \"CONTENT\": \"damn nvm what I said\"\n",
    "}\n",
    "\n",
    "# Send updates to 'youtube_update' topic\n",
    "future = producer.send(\"youtube_update\", message)\n",
    "future.add_callback(on_success)\n",
    "future.add_errback(on_error)\n",
    "\n",
    "# Ensure all messages are sent before exiting\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3b74bdf-5096-48ce-8338-0f03029342c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|        _c1|\n",
      "+-----------+\n",
      "|         69|\n",
      "|        ass|\n",
      "|       fuck|\n",
      "|       fuck|\n",
      "|       fuck|\n",
      "|        ass|\n",
      "|        sex|\n",
      "|        sex|\n",
      "|     orgasm|\n",
      "|     orgasm|\n",
      "|ejaculation|\n",
      "|       arse|\n",
      "|       arse|\n",
      "|       arse|\n",
      "|   foreskin|\n",
      "|       shit|\n",
      "|       shit|\n",
      "|      skank|\n",
      "|        ass|\n",
      "|    abraham|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Download profane words dataset: https://www.kaggle.com/datasets/konradb/profanities-in-english-collection\n",
    "path = \"profanity_en.csv\"\n",
    "df = spark.read.csv(path)\n",
    "profane_df = df.select(df._c1)\n",
    "profane_df = profane_df.filter(df._c1 != 'canonical_form_1')\n",
    "profane_df.show()\n",
    "\n",
    "profane_set = set(profane_df.select(profane_df._c1).rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "174a8304-371f-4c36-ab43-d2620d756d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from kafka and mask profane words from content\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"COMMENT_ID\", StringType(), True),\n",
    "    StructField(\"CONTENT\", StringType(), True)\n",
    "])\n",
    "\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"youtube_update\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# data = {\n",
    "#     \"COMMENT_ID\" : \"z123std54m2ozht10232efr5svb4vh0au04\",\n",
    "#     \"CONTENT\": \"damn nvm what I said\"\n",
    "# }\n",
    "\n",
    "json_stream = kafka_stream.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a71bd6c8-4c95-4b91-b6b2-eb7d73a7e72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/05 01:23:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/10/05 01:23:56 WARN StreamingQueryManager: Stopping existing streaming query [id=2390ed90-ff92-430c-a198-29e5e13fb07b, runId=c62c7339-749a-4c07-b1ab-913b7c9ba947], as a new run is being started.\n",
      "24/10/05 01:23:56 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.20/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py\", line 716, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 40\u001b[0m\n\u001b[1;32m     29\u001b[0m     new_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morg.elasticsearch.spark.sql\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mes_conf) \\\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myoutube\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m query \u001b[38;5;241m=\u001b[39m json_stream\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;241m.\u001b[39mforeachBatch(mask_profane_word_df) \\\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mes_conf) \\\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;241m.\u001b[39mstart(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoutube\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/streaming/query.py:201\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.20/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Update the DataFrame to an Elasticsearch index\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "es_conf = {\n",
    "    \"es.nodes.discovery\": \"false\",\n",
    "    \"es.nodes.data.only\": \"false\",\n",
    "    \"es.net.http.auth.user\": \"elastic\",\n",
    "    \"es.net.http.auth.pass\": \"password\",\n",
    "    \"es.nodes\": \"http://127.0.0.1\",\n",
    "    \"es.port\": \"9200\",\n",
    "    \"es.mapping.id\": \"COMMENT_ID\",\n",
    "    \"es.mapping.exclude\": \"COMMENT_ID\",\n",
    "    \"es.write.operation\": \"update\",\n",
    "    \"checkpointLocation\": \"/tmp/\",\n",
    "    \"es.spark.sql.streaming.sink.log.enabled\": \"false\",\n",
    "}\n",
    "\n",
    "def mask_profane_word(s):\n",
    "    words = s.split()\n",
    "    tokens = [t if t not in profane_set else '****' for t in words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def mask_profane_word_df(df, epoch_id):\n",
    "    name = 'CONTENT'\n",
    "    udf = UserDefinedFunction(lambda x: mask_profane_word(x), StringType())\n",
    "    new_df = df.select(*[udf(column).alias(name) if column == name else column for column in df.columns])\n",
    "    \n",
    "    new_df.write.mode(\"append\") \\\n",
    "        .format('org.elasticsearch.spark.sql') \\\n",
    "        .options(**es_conf) \\\n",
    "        .save('youtube')\n",
    "   \n",
    "query = json_stream.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .foreachBatch(mask_profane_word_df) \\\n",
    "        .options(**es_conf) \\\n",
    "        .start(\"youtube\")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11496e31-2df9-4a4c-91d0-430db87012a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"took\":17,\"timed_out\":false,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":579,\"relation\":\"eq\"},\"max_score\":12.020087,\"hits\":[{\"_index\":\"youtube\",\"_id\":\"z123std54m2ozht10232efr5svb4vh0au04\",\"_score\":12.020087,\"_source\":{\"COMMENT_ID\":\"z123std54m2ozht10232efr5svb4vh0au04\",\"AUTHOR\":\"MB LOVE\",\"DATE\":\"2014-08-13T03:04:34\",\"CONTENT\":\"damn, nvm what I said\",\"VIDEO_NAME\":\"Katy Perry - Roar\",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"z13rzhjaxxr3z1t4504ccvlp1uqnddvqoyg0k\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"z13rzhjaxxr3z1t4504ccvlp1uqnddvqoyg0k\",\"AUTHOR\":\"Kate Love\",\"DATE\":\"2015-05-23T01:14:31.812000\",\"CONTENT\":\"Best song﻿\",\"VIDEO_NAME\":\"Eminem - Love The Way You Lie ft. Rihanna\",\"CLASS\":\"0\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc6_onwOgxju-DV6WkqHZEOztCXD04EgEFBU\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc6_onwOgxju-DV6WkqHZEOztCXD04EgEFBU\",\"AUTHOR\":\"Hidden Love\",\"DATE\":\"2013-09-24T14:29:54.072000\",\"CONTENT\":\"Hi. Check out and share our songs.\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc69r15LuL8TDbisnTJ_hf5RfcyJAyoMC5eo\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc69r15LuL8TDbisnTJ_hf5RfcyJAyoMC5eo\",\"AUTHOR\":\"Hidden Love\",\"DATE\":\"2013-08-06T11:40:05.581000\",\"CONTENT\":\"Hi. Check out and share our songs.\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc68Qq98m0mmx4rlprYiD6aYgMb2x3bdupEM\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc68Qq98m0mmx4rlprYiD6aYgMb2x3bdupEM\",\"AUTHOR\":\"Hidden Love\",\"DATE\":\"2013-08-01T09:19:56.654000\",\"CONTENT\":\"Hi. Check out and share our songs.\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc698fLCQzZmRt1CqPhf0L6mchwWG5gsUlzk\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc698fLCQzZmRt1CqPhf0L6mchwWG5gsUlzk\",\"AUTHOR\":\"Hidden Love\",\"DATE\":\"2013-07-31T10:22:02.628000\",\"CONTENT\":\"Hi. Check out and share our songs.\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc68sxLpsBQ1cPk_LLH91SMsRav51KmhIoQw\",\"_score\":5.402604,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc68sxLpsBQ1cPk_LLH91SMsRav51KmhIoQw\",\"AUTHOR\":\"Hidden Love\",\"DATE\":\"2013-07-30T11:22:57.091000\",\"CONTENT\":\"Hi.Check out and share our songs.\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"z13bvpnptyiuzjlr004cijkhguzvjlsjrpw0k\",\"_score\":3.9471169,\"_source\":{\"COMMENT_ID\":\"z13bvpnptyiuzjlr004cijkhguzvjlsjrpw0k\",\"AUTHOR\":\"jen ven\",\"DATE\":\"2015-05-18T18:14:56.234000\",\"CONTENT\":\"Love these guys, love the song!﻿\",\"VIDEO_NAME\":\"LMFAO - Party Rock Anthem ft. Lauren Bennett, GoonRock\",\"CLASS\":\"0\"}},{\"_index\":\"youtube\",\"_id\":\"_2viQ_Qnc69JZ1f3TJQWKMLhmdP0inDDPwOvzMQiFjE\",\"_score\":3.8681548,\"_source\":{\"COMMENT_ID\":\"_2viQ_Qnc69JZ1f3TJQWKMLhmdP0inDDPwOvzMQiFjE\",\"AUTHOR\":\"Su Xianfeng\",\"DATE\":\"2013-10-05T11:54:10.192000\",\"CONTENT\":\"subscribe now!!!!!! love the song!!!!!! love football!!!\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"1\"}},{\"_index\":\"youtube\",\"_id\":\"z123eldrazr3wlyd222zitmg1y2jin1rf04\",\"_score\":3.821743,\"_source\":{\"COMMENT_ID\":\"z123eldrazr3wlyd222zitmg1y2jin1rf04\",\"AUTHOR\":\"Mahya Roohi\",\"DATE\":\"2015-05-21T08:19:31.099000\",\"CONTENT\":\"love!!!!﻿\",\"VIDEO_NAME\":\"Shakira - Waka Waka \",\"CLASS\":\"0\"}}]}}"
     ]
    }
   ],
   "source": [
    "! curl -X GET http://localhost:9200/youtube/_search\\?q\\=MB%20LOVE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3.9.20",
   "language": "python",
   "name": "python-3.9.20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
